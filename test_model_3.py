import time
from llama_cpp import Llama

print("Загрузка модели (GPU)...")
llm = Llama(
    model_path=r"E:\Projects\MudMind\data\models\micro\mudreflex-core.gguf",
    n_gpu_layers=-1,
    n_ctx=4096,
    n_batch=512,
    verbose=True  # убираем огромный лог
)
print("Модель загружена\n")

prompt = ("Вот описание локации из MUD:\n"
            "Добро пожаловать на землю Киевскую, богатую историей и самыми невероятными приключениями. "
            "Возможно, вам они понравятся, и вы станете в один ряд с героями давно минувших дней.\n\n"
            "Комнаты отдыха\n"
            "Хозяин устроил здесь комнаты для отдыха. Любой желающий может остановиться здесь и передохнуть после дальней дороги. "
            "Светлые горницы так и приглашают пройти в них и растянуться на перине, на пуховой да забыться сном богатырским. "
            "Здесь же можно подождать своих компаньонов перед долгой дорогой, поговорить о дальнейших планах.\n"
            "Совсем малых, да не обученных так и тянет войти в школу.\n"
            "Доска для различных заметок и объявлений прибита тут ..блестит!\n"
            "Хозяйка постоялого двора распоряжается здесь.\n"
            "Статистика игрока: 31H 85M 1999о Зауч:0 ОЗ:0 1L 0G Вых:v>")

start = time.perf_counter()

result = llm.create_chat_completion(
    messages=[
        {"role": "system", "content": "Ты — помощник игрока в текстовой MUD. Анализируй описание комнаты, объектов, NPC и событий. Предлагай действия, оценивай риски, пиши кратко и по сути, только на основе текста MUD. Давай ТОЛЬКО 3-4 конкретные команды для игрока."},
        {"role": "user", "content": prompt}
    ],
    max_tokens=8200,
    temperature=0.3,
)

end = time.perf_counter()

print("=== Ответ модели ===")
print(result["choices"][0]["message"]["content"])
print("\n=== Метрики ===")
print(f"Tokens generated  : {result['usage']['total_tokens']}")
print(f"Generation time   : {end - start:.2f} sec")

